{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1. Regularization\n",
    "\n",
    "以下两种写法是等价的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# 写法一：手工添加正则化项\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(input=outputs, target=labels)\n",
    "\n",
    "            # Regularization\n",
    "            l2_lambda = 0.001  # 正则项系数\n",
    "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
    "            loss += l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"epoch: {epoch} training loss: {total_loss / len(train_loader)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Linear(3, 5))\n",
    "\n",
    "# 写法二：创建optimizer时传入weight_decay参数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, weight_decay=0.01)\n",
    "# 注意这里weight_decay的值应该等于2 * l2_lambda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Dropout\n",
    "\n",
    "Dropout的流程：\n",
    "\n",
    "每一个mini-batch都在训练不同的网络：\n",
    "1. 随机删除网络中一定比例的隐藏层的神经元（输入输出神经元不变）。\n",
    "2. 将输入x通过修改后的网络前向传播。\n",
    "3. 将得到的损失通过修改后的网络反向传播。\n",
    "4. 重复这一过程。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.9123, -0.2944, -0.4776],\n        [-0.0000,  1.3038,  0.0000],\n        [-0.4803,  1.6488,  0.0000],\n        [ 0.0773, -0.5763, -1.1433],\n        [-0.0206,  0.1642, -1.1635]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropout函数形式，用于演示\n",
    "dropout = nn.Dropout(p=0.1)  # p是丢弃的概率\n",
    "input = torch.randn(5, 3)\n",
    "output = dropout(input)\n",
    "output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Dropout类的形式，用在Sequential中\n",
    "dropout_net = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(p=0.4),\n",
    "\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(p=0.4),\n",
    "\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.Dropout(p=0.4),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2),\n",
    ")\n",
    "\n",
    "img = torch.randn(1000, 3, 32, 32)\n",
    "\n",
    "# 将模型状态设为“训练”，启用Dropout\n",
    "dropout_net.train()\n",
    "out = dropout_net(img)\n",
    "print(out.shape)\n",
    "\n",
    "# 将模型状态设为“测试”，停用Dropout\n",
    "dropout_net.eval()\n",
    "out = dropout_net(img)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Batch Normalization\n",
    "\n",
    "Batch Normalization的主要思想：调整每一层输入到激活函数的输入值，\n",
    "使其满足某个分布，以防止梯度消失。\n",
    "\n",
    "Batch Normalization的好处：允许使用更高的学习率、\n",
    "使训练过程更少依赖于初始值、可以代替Dropout。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "# Batch Normalization一般使用在激活函数以前\n",
    "bn_net = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(16),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "    nn.BatchNorm2d(8),\n",
    "    nn.Tanh(),\n",
    "    nn.MaxPool2d(2),\n",
    "\n",
    "    nn.Flatten(),\n",
    "\n",
    "    nn.Linear(8 * 8 * 8, 32),\n",
    "    nn.BatchNorm1d(32),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(32, 2),\n",
    ")\n",
    "\n",
    "img = torch.randn(1000, 3, 32, 32)\n",
    "\n",
    "# 将模型状态设为“训练”\n",
    "bn_net.train()\n",
    "out = bn_net(img)\n",
    "print(out.shape)\n",
    "\n",
    "# 将模型状态设为“测试”，Batch Normalization的规则与训练时有所不同\n",
    "bn_net.eval()\n",
    "out = bn_net(img)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-418948df",
   "language": "python",
   "display_name": "PyCharm (ACM-Py)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}